Generating Wildfire data using Denoising Diffusion Probabilistic Models

For this project, the main objective is to predict wildfire spread through the latent space. In order to do this, I first trained a denoising diffusion probabilistic model on a wildfire dataset. The specific dataset I used was the Kaggle Next Day Wildfire Spread dataset. I used this dataset due to the convenient accumulation of all the important environmental data. The dataset features close to 15,000 images each of size 64x64 with 13 different data layers. Many of the other datasets I was looking at were difficult to download as they were hosted on Government websites with a GUI suited to download particular wildfire perimeters, and not suited for mass downloads. Initially, I attempted to create my own diffusion model, but after some effort, I decided it would not be efficient enough and wouldn't be integral to this project if an off-the-shelf implementation met all my needs. I went with an implementation of the Dec. 2020 Berkley paper titled Denoising Diffusion Probabilistic Models by Jonathan Ho et. all. The implementation offered a good balance of relevance, efficiency, and readability. I conducted several training experiments to see how much data I could train on and get results in a reasonable time. At first I tried to train the model with all 13 data layers as different channels at the full 64x64 resolution. After 12 hours of training on a cloud based RTX 4090 the results were not similar to the training dataset, so I did not further this approach. My next test was if I could generate a minimum amount of data. I trained the model on just 3 data layers (elevation, previous fire mask, and current fire mask) each in their own channel down sampled to 32x32. After 12 hours of training on that same RTX 4090 the results were very good, and each generated data layer showed clear signs that it represented the training dataset. My next test was to increase the amount of data layers and to see if adding them all to one channel would increase performance. I took nine data layers (elevation, wind direction, wind velocity, precipitation, drought, vegetation, population density, previous fire mask and current fire mask) and tiled them into a 9x9 to make a 96x96 image with 1 channel. After 12 hours of training on the same RTX 4090 I got similar results to my first test that were unsubstantial. Knowing that training with 3 channels worked I then chose to train the model on 6 data layers (elevation, vegetation, wind direction, wind speed, previous fire mask and current fire mask). I tiled two data layers together for each channel so that each channel has a resolution of 32x64. After 12 hours I got promising results and decided to train for 12 more hours for a total of 24 hours on the same cloud based RTX 4090 GPU. After sampling, I obtained very reasonable results; when compared to the training dataset, they looked almost identical. The metric I used to evaluate the performance of the data generated was FID score. The 6 data layer test generated samples with an FID score of 5.4, while the failed attempts generated sampled with FID scores around 50. After training the diffusion model on a suitable subset of the entire dataset, my next task was to do interpolation. This is where I hit a major roadblock. When interpolation the data, the diffusion model would make nonsensical data for either a selection of the data layers, or all of them. In addition, some data layers would be entirely blank. This was surprising to me since the model had no problem generation those data layers before. As I was unable to do interpolation, I was unable to do prediction in the latent space. This being said, the generation of this type of environmental wildfire data is substantial. As far as I am concerned, there is very little work done with diffusion models in the field of wildfire research. The only other paper I could find that used diffusion models on wildfire data was to generate photorealistic images of wildfires, such as trees burning. The main aim of this being to train wildfire detection models, rather than wildfire spread models. The generation of wildfire data is suitable for modeling wildfires that are less common, but of higher interest such as larger wildfire or wildfires close to points of interest such as towns or infrastructure. My next steps for this project would be to use a model such as stable diffusion that allows for conditioning of environmental elements and just generating the wildfire perimeters itself. This would also aid in the interpolation as I could keep the environmental data constant while only interpolating the wildfire perimeters. 

Files Descriptions: 
archive (directory): Contains the files for the original Kaggle dataset.
interpolated (directory): Contains interpolated images (Use 1x2x3_image_viewer to view data layers)
results (directory): Contains saved model parameters from training.
Samples (directory): Contains images generated by DDPM.
WildFireImages1x2x3 (directory): Contains the dataset with 6 data layers. This is the main dataset I used
WildFireImages1x3 (directory): Contains the dataset with 3 data layers.
WildFireImages3x3 (directory): Contains the dataset with 9 data layers.
1x2x3_image_viewer.py: Python script to view images with 6 data layers.
1x2x3_images.zip: Zip file containing WildFireImages1x2x3 (directory).
32x32x1_images.zip: Zip file containing WildFireImages1x3 (directory).
dataset_1x1x3.ipynb: Jupiter notebook to generate WildFireImages1x3 (directory).
dataset_1x2x3.ipynb: Jupiter notebook to generate WildFireImages1x2x3 (directory).
dataset_3x3x1.ipynb: Jupiter notebook to generate WildFireImages3x3 (directory).
environment.yml: The conda environment file for running all files.
interpolate.py: Python script to generate interpolated images.
sample_image.py: Python script to generate and view sample images.
train_diffusion.py: Python script to train DDPM model
unzip.py: Python script to unzip files.

Main Files of Importance: 
dataset_1x2x3.ipynb, train_diffusion.py, sample_image.py, interpolate.py, environment.yml
 
